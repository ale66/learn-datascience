{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aj_-zCoDc7Sx"
   },
   "source": [
    "### Chapter IV - WWW, Wiki and Online social networks.\n",
    "\n",
    "#### This __exercise__ notebook is taken from the Python 2 notebook for Ch. 4 of Caldarelli-Cheesa's textbook (CC).\n",
    "\n",
    "The challege questions at the bottom are solved in the `solution` version of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8_dm3ETc7S2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZF8-H05c7S4"
   },
   "source": [
    "#### Get data from The Laboratory for Web Algorithmics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LKi2A_3c7S5"
   },
   "source": [
    "#### This is the page with the datasets: http://law.di.unimi.it/datasets.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwLCfr1Oc7S5"
   },
   "source": [
    "It is possible to download a network in a WebGraph format that is a compressed binary format.\n",
    "\n",
    "The project provides various clients to extract the network strcture, in Java, C++ and in Python, py-web-graph: http://webgraph.di.unimi.it/.\n",
    "\n",
    "In particular we got the graph and the related urls associated to each node of the .eu domain in 2005: http://law.di.unimi.it/webdata/eu-2005/.\n",
    "\n",
    " We exctracted the graph in a form of an edge list and we also got the file with the list of urls in the same order of the node_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Colab execution:\n",
    "\n",
    "Please upload the two data files shown below to your `sample_data` folder in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQDGA-p0c7S6"
   },
   "outputs": [],
   "source": [
    "ARCSFILE = './sample_data/eu-2005_1M.arcs'\n",
    "\n",
    "URLSFILE = './sample_data/eu-2005.urls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzZ3urAnc7S6"
   },
   "outputs": [],
   "source": [
    "#retrieve just the portion of the first 1M edges of the .eu domain\n",
    "#crawled in 2005\n",
    "eu_DG = nx.read_edgelist(ARCSFILE, create_using = nx.DiGraph())\n",
    "\n",
    "#generate the dictionary of node_id -> urls\n",
    "file_urls = open(URLSFILE)\n",
    "\n",
    "count = 0\n",
    "\n",
    "dic_nodid_urls = {}\n",
    "\n",
    "while True:\n",
    "    next_line = file_urls.readline()\n",
    "\n",
    "    if not next_line:\n",
    "        break\n",
    "\n",
    "    dic_nodid_urls[str(count)] = next_line[ :-1]\n",
    "    count = count+1\n",
    "\n",
    "file_urls.close()\n",
    "\n",
    "#generate the strongly connected component\n",
    "scc = [(len(c),c) for c in sorted( nx.strongly_connected_components \\\n",
    "                               (eu_DG), key=len, reverse=True)][0][1]\n",
    "\n",
    "eu_DG_SCC = eu_DG.subgraph(scc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uByMvtBvc7S7"
   },
   "outputs": [],
   "source": [
    "l = [e for e in eu_DG_SCC.edges]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0I6GeVEc7S8"
   },
   "outputs": [],
   "source": [
    "l[ :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OS4IHiMc7S9"
   },
   "source": [
    "#### Retrieving data through the  [Twitter API](https://dev.twitter.com/docs) usign the [Twython](http://twython.readthedocs.org/en/latest/) module\n",
    "\n",
    "This part is not in use anymore as the TwitterAPI does not generally serve data anymore: we get a `403` error.\n",
    "\n",
    "Please proceed to the 'HITS algorithm' section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlB8sLkYc7S9"
   },
   "source": [
    "## Hits algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHm_zXXMc7S9"
   },
   "source": [
    "##### Create a simple labeled network: the 'four triangles' network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxYNoBDOc7S-"
   },
   "outputs": [],
   "source": [
    "DG = nx.DiGraph()\n",
    "\n",
    "DG.add_edges_from([('A','B'),('B','C'),('A','D'),\n",
    "                   ('D','B'),('C','D'),('C','A')])\n",
    "\n",
    "#plot the graph\n",
    "nx.draw(DG, with_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BMCAluRc7S-"
   },
   "source": [
    "The network has a certain symmetry: each node has in-degree of 2 and out-degree of 1 or vice versa.\n",
    "\n",
    "\n",
    "#### Direct implementation of the [HITS algorithm](https://en.wikipedia.org/wiki/HITS_algorithm) by [Kleinberg](https://en.wikipedia.org/wiki/Jon_Kleinberg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gP3tiVfzc7S-"
   },
   "outputs": [],
   "source": [
    "def HITS_algorithm(digraph: nx.DiGraph, K: int = 1000) -> tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "    :param digraph: A networkx DiGraph\n",
    "    :param K: The K maximum number of iterations\n",
    "\n",
    "    :return: Two dictionaries containing the hub and authority scores, resp.\n",
    "    \"\"\"\n",
    "    auth_scores = {}\n",
    "    hub_scores = {}\n",
    "\n",
    "    for n in digraph.nodes():\n",
    "        auth_scores[n] = 1.0\n",
    "        hub_scores[n] = 1.0\n",
    "\n",
    "    for k in range(K):\n",
    "        norm = 0.0\n",
    "\n",
    "        for n in digraph.nodes():\n",
    "            auth_scores[n] = 0.0\n",
    "\n",
    "            # REMINDER: a predecessor of a node n is a node m\n",
    "            # such that there is a direct edge from m to n\n",
    "            for p in digraph.predecessors(n):\n",
    "                auth_scores[n] += hub_scores[p]\n",
    "\n",
    "            norm += auth_scores[n]**2.0\n",
    "\n",
    "        norm = norm**0.5\n",
    "\n",
    "        for n in digraph.nodes():\n",
    "            auth_scores[n] = auth_scores[n] / norm\n",
    "\n",
    "        norm = 0.0\n",
    "\n",
    "        for n in digraph.nodes():\n",
    "            hub_scores[n] = 0.0\n",
    "\n",
    "            for s in digraph.successors(n):\n",
    "                hub_scores[n] += auth_scores[s]\n",
    "\n",
    "            norm += hub_scores[n]**2.0\n",
    "\n",
    "        norm = norm**0.5\n",
    "\n",
    "        for n in digraph.nodes():\n",
    "            hub_scores[n] = hub_scores[n] / norm\n",
    "\n",
    "        return auth_scores, hub_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Iag8XZzc7S-"
   },
   "source": [
    "#### Let's put HITS to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWIT3WAzc7S_"
   },
   "outputs": [],
   "source": [
    "(auth, hub) = HITS_algorithm(DG, K=100)\n",
    "\n",
    "print (auth)\n",
    "print (hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdLjwRnuc7S_"
   },
   "source": [
    "### Q1.  Use built in hits function to find hub and authority scores.\n",
    "\n",
    "Can you spot the differences in result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCGpt9cOc7S_"
   },
   "outputs": [],
   "source": [
    "nx.draw_networkx(DG, with_labels = True)\n",
    "\n",
    "# your solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwX8gLGhc7S_"
   },
   "source": [
    "#### Adjacency matrix representation with basic operations\n",
    "\n",
    "We refrain from using the standard `Numpy` methods for transposing and multiplying matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEAb39xAc7TA"
   },
   "outputs": [],
   "source": [
    "def matrix_transpose(M: list) -> list:\n",
    "\n",
    "    M_out=[]\n",
    "\n",
    "    for c in range(len(M[0])):\n",
    "\n",
    "        M_out.append([])\n",
    "\n",
    "        for r in range(len(M)):\n",
    "            M_out[c].append(M[r][c])\n",
    "\n",
    "    return M_out\n",
    "\n",
    "\n",
    "def matrix_multiplication(M1: list, M2: list) -> list:\n",
    "\n",
    "    M_out=[]\n",
    "\n",
    "    for r in range(len(M1)):\n",
    "\n",
    "        M_out.append([])\n",
    "\n",
    "        for j in range(len(M2[0])):\n",
    "            e=0.0\n",
    "\n",
    "            for i in range(len(M1[r])):\n",
    "                e+=M1[r][i]*M2[i][j]\n",
    "\n",
    "            M_out[r].append(e)\n",
    "\n",
    "    return M_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0qHfS9jc7TA"
   },
   "source": [
    "Now, let's test the home-brew functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSoj72Jcc7TB"
   },
   "outputs": [],
   "source": [
    "\n",
    "adjacency_matrix1=[\n",
    "                  [0,1,0,1],\n",
    "                  [1,0,1,1],\n",
    "                  [0,1,0,0]\n",
    "                  ]\n",
    "\n",
    "adjacency_matrix2 = matrix_transpose(adjacency_matrix1)\n",
    "\n",
    "print (\"Transpose adjacency matrix:\", adjacency_matrix2)\n",
    "\n",
    "res_mul = matrix_multiplication(adjacency_matrix1, adjacency_matrix2)\n",
    "\n",
    "print (\"Matrix multiplication:\", res_mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "341nFPeuc7TB"
   },
   "source": [
    "Differently from the `Numpy` methods, our functions work with pure lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gbm_YO0Dc7TC"
   },
   "outputs": [],
   "source": [
    "type(res_mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DbelI74c7TC"
   },
   "source": [
    "### The Power-iterations algorithm: a direct implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecMjG_NOc7TD"
   },
   "outputs": [],
   "source": [
    "adjacency_matrix=[\n",
    "                  [0,1,0,1],\n",
    "                  [1,0,1,1],\n",
    "                  [0,1,0,0],\n",
    "                  [1,1,0,0]\n",
    "                  ]\n",
    "vector=[\n",
    "        [0.21],\n",
    "        [0.34],\n",
    "        [0.52],\n",
    "        [0.49]\n",
    "        ]\n",
    "\n",
    "# For small examples, few iterations will be needed.\n",
    "C = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsbrgQl_c7TD"
   },
   "outputs": [],
   "source": [
    "for i in range(C):\n",
    "    res = matrix_multiplication(adjacency_matrix, vector)\n",
    "\n",
    "    norm_sq = 0.0\n",
    "\n",
    "    for r in res:\n",
    "        norm_sq = norm_sq+r[0]*r[0]\n",
    "\n",
    "    vector = []\n",
    "\n",
    "    for r in res:\n",
    "         vector.append([r[0]/(norm_sq**0.5)])\n",
    "\n",
    "print (\"Maximum eigenvalue (in absolute value):\", norm_sq**0.5)\n",
    "print (\"Eigenvector for the maximum eigenvalue:\", vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MEiTsHvc7TD"
   },
   "source": [
    "#### Putting it all together: computing HITS for the WWW strongly-connected component of the `.eu` domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgeSsO8Dc7TD"
   },
   "outputs": [],
   "source": [
    "# Use operator.itemgetter(1) to sort the dictionary by value\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FprcchM8c7TD"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "#Please assign your results to lists sorted_auth and sorted_hub, respectively.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, uncomment as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGJn9ooBc7TE"
   },
   "outputs": [],
   "source": [
    "#top ranking auth\n",
    "print (\"Top 5 by auth\")\n",
    "\n",
    "#for p in sorted_auth[:5]:\n",
    "#    print (dic_nodid_urls[p[0]], p[1])\n",
    "\n",
    "#top ranking hub\n",
    "print (\"Top 5 by hub\")\n",
    "\n",
    "#for p in sorted_hub[:5]:\n",
    "#   print (dic_nodid_urls[p[0]], p[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2n3sq9fc7TE"
   },
   "source": [
    "### Q2. Run the built-in `nx.hits` function; can you spot the differences in result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o48XCoRmc7TE"
   },
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "#Please assign your results to lists sorted_auth and sorted_hub, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45GrCrp8c7TE"
   },
   "outputs": [],
   "source": [
    "# print (\"Top-5 auth nodes:\")\n",
    "\n",
    "# for p in sorted_auth[:5]:\n",
    "#     print (dic_nodid_urls[p[0]], p[1])\n",
    "#     print (\"Top-5 hub nodes:\")\n",
    "\n",
    "# for p in sorted_hub[:5]:\n",
    "#     print (dic_nodid_urls[p[0]], p[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRhyOPNCc7TE"
   },
   "source": [
    "#### Compute the PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PB-Rdj2ac7TE"
   },
   "outputs": [],
   "source": [
    "def pagerank(graph, damping_factor = 0.85, max_iterations = 100, min_delta = 0.00000001):\n",
    "\n",
    "    nodes = graph.nodes()\n",
    "\n",
    "    graph_size = len(nodes)\n",
    "\n",
    "    if graph_size == 0:\n",
    "        return {}\n",
    "\n",
    "    # itialize the page rank dict with 1/N for all nodes\n",
    "    page_rank = dict.fromkeys(nodes, (1.0-damping_factor)*1.0/ graph_size)\n",
    "\n",
    "    min_value = (1.0-damping_factor)/len(nodes)\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        #total difference compared to last iteraction\n",
    "        diff = 0\n",
    "\n",
    "        # computes each node PageRank based on inbound links\n",
    "        for node in nodes:\n",
    "            rank = min_value\n",
    "\n",
    "            for referring_page in graph.predecessors(node):\n",
    "                rank += damping_factor * page_rank[referring_page]/ \\\n",
    "                len(list(graph.neighbors(referring_page)))\n",
    "\n",
    "            diff += abs(page_rank[node] - rank)\n",
    "\n",
    "            page_rank[node] = rank\n",
    "\n",
    "        #stop if PageRank has converged\n",
    "        if diff < min_delta:\n",
    "            break\n",
    "\n",
    "    return page_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxbmZHyhc7TF"
   },
   "source": [
    "#### The Networkx version of [PageRank](http://networkx.github.io/documentation/latest/reference/generated/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqNhPQe0c7TF"
   },
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "G.add_edges_from([(1, 2), (2, 3), (3, 4), (3, 1), (4, 2)])\n",
    "#plot the network\n",
    "\n",
    "nx.draw(G, with_labels = True)\n",
    "\n",
    "#our Page Rank algorithm\n",
    "res_pr=pagerank(G, max_iterations = 10000, min_delta = 0.00000001, damping_factor = 0.85)\n",
    "print (res_pr)\n",
    "\n",
    "#Networkx Pagerank function\n",
    "print (nx.pagerank(G,max_iter = 10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUxMUmTjc7TG"
   },
   "source": [
    "#### The Twitter Mention Network\n",
    "\n",
    "Please skip this section as we don't access Twitter/X data anymore; proceed to he `Scwiki` section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Community detection I: the Karate Club\n",
    "\n",
    "Let visualise a prediction of the community structure of the Karate Club network *before the split*.\n",
    "\n",
    "ideally, we expect two communities (or *factions*), one around node 1 (the president of the club) and one around node 34 (the karate master).\n",
    "\n",
    "We will operate the `Louvain` community detection algorithm<!-- .slide: data-fullscreen -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFILE = './sample_data/karate.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist(DATAFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run community detection. \n",
    "\n",
    "The Leuven algorithm, which we have not yet seen in class, has a random component, so it makes sense to run it a few times to seek a *clean* visualisation. \n",
    "\n",
    "Notice how, since we know what to expect from the Karate Club, we are defaulting on a sort of supervised version of the community detection problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try running it mopre than once to see how the results change\n",
    "partition = community.best_partition(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result to see how close to the expected result we are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#plot the network\n",
    "size = float(len(set(partition.values())))\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "count = 0.\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "for com in set(partition.values()) :\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    list_nodes = [nodes for nodes in partition.keys() \\\n",
    "                  if partition[nodes] == com]\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, list_nodes, node_size = 300, \\\n",
    "                           node_color = str(count / size))\n",
    "    \n",
    "    nx.draw_networkx_labels(G,pos)\n",
    "\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5, width=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result visualization\n",
    "\n",
    "plt.savefig('./sample_data/karate_community.png', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MF10dVRWc7TG"
   },
   "source": [
    "#### Community Detection for the `Scwiki` network\n",
    "\n",
    "Wikipedia is public and very connected, both internally (Wikipedia links) and externally (links to other sites).\n",
    "\n",
    "It is interesting to see whether the links connecting two pages (lemmata of the encyclopedia) determine some community of concepts and, ultimately, a bottom-up, data/user-driven taxonomy of concepts, as in *curated* scientific taxonomies etc.\n",
    "\n",
    "The present shape of the Wikipedia is available from [dumps.wikimedia.org](https://dumps.wikimedia.org/).\n",
    "\n",
    "The data presented here is a tiny fragment of the *Sardininan* Wikipedia, which is developed for the spoken language of [Sardinia](https://en.wikipedia.org/wiki/Sardinia): \n",
    "\n",
    "[sc.wikipedia.org/](https://sc.wikipedia.org/)\n",
    "\n",
    "\n",
    "#### References:\n",
    "\n",
    "- for the structure of the *PageLinks* table files: [www.mediawiki.org/wiki?Manual:Page_links_table](https://www.mediawiki.org/wiki?Manual:Page_links_table)\n",
    "\n",
    "- for the structure of the Page table files: [www.mediawiki.org/wiki?Manual:Page_table](https://www.mediawiki.org/wiki?Manual:Page_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyegjqF5c7TG"
   },
   "outputs": [],
   "source": [
    "SCWIKI = './sample_data/scwiki_edgelist.dat'\n",
    "\n",
    "TITLES = './sample_data/scwiki_page_titles.dat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJPotZauc7TH"
   },
   "source": [
    "Warning: in `.eu` there are pages in the Sardinian language (and perhaps others) which require a specific coding on your own platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2T0y9ckc7TH"
   },
   "outputs": [],
   "source": [
    "#load the directed and undirected version og the scwiki graph\n",
    "scwiki_pagelinks_net_dir = nx.read_edgelist(SCWIKI, create_using = nx.DiGraph())\n",
    "\n",
    "scwiki_pagelinks_net = nx.read_edgelist(SCWIKI)\n",
    "\n",
    "#load the page titles\n",
    "dict_titles = {}\n",
    "\n",
    "file_titles = open(TITLES, 'r', encoding='utf-8')\n",
    "\n",
    "while True:\n",
    "    next_line = file_titles.readline()\n",
    "\n",
    "    if not next_line:\n",
    "        break\n",
    "\n",
    "    print (next_line.split()[0], next_line.split()[1])\n",
    "\n",
    "    dict_titles[next_line.split()[0]] = next_line.split()[1]\n",
    "\n",
    "file_titles.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to partition the Sardinian Wikipedia network into communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = community.best_partition(scwiki_pagelinks_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a new network is created, which represents communities of concepts and their connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate representative nodes of the community structure\n",
    "community_structure = nx.Graph()\n",
    "\n",
    "diz_communities={}\n",
    "diz_node_labels={}\n",
    "diz_node_sizes={}\n",
    "max_node_size=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for com in set(partition.values()) :\n",
    "    diz_communities[com] = [nodes for nodes in partition.keys() \\\n",
    "                            if partition[nodes] == com]\n",
    "    \n",
    "    if len(diz_communities[com])>=200:\n",
    "        if max_node_size<len(diz_communities[com]):\n",
    "            max_node_size=len(diz_communities[com])\n",
    "        print(\"community\",com,len(diz_communities[com]), end=' ')\n",
    "    \n",
    "        sub_scwiki_dir = scwiki_pagelinks_net_dir.subgraph \\\n",
    "        (diz_communities[com])    \n",
    "    \n",
    "        res_pr=nx.pagerank(sub_scwiki_dir,max_iter=10000)\n",
    "    \n",
    "        sorted_pr=sorted(res_pr.items(), key=operator.itemgetter \\\n",
    "                         (1),reverse=True)\n",
    "\n",
    "        print(dict_titles[sorted_pr[0][0]],sorted_pr[0][1])\n",
    "    \n",
    "        community_structure.add_node(com)\n",
    "\n",
    "        diz_node_labels[com] = dict_titles[sorted_pr[0][0]]\n",
    "    \n",
    "        diz_node_sizes[com] = len(diz_communities[com])\n",
    "       \n",
    "#Generate edge weights according to the number of links\n",
    "#among communities\n",
    "max_edge_weight=0.0\n",
    "\n",
    "for i1 in range(community_structure.number_of_nodes()-1):\n",
    "    \n",
    "    for i2 in range(i1+1,community_structure.number_of_nodes()):\n",
    "        wweight=0.0\n",
    "        \n",
    "        for n1 in diz_communities[list(community_structure.nodes())[i1]]:\n",
    "            for n2 in diz_communities[list(community_structure.nodes()) \\\n",
    "                                      [i2]]:\n",
    "                if scwiki_pagelinks_net.has_edge(n1,n2):\n",
    "                    wweight += 1.0\n",
    "                    \n",
    "        if wweight>100.0:\n",
    "            if max_edge_weight<wweight:\n",
    "                max_edge_weight=wweight\n",
    "                \n",
    "            community_structure.add_edge(list(community_structure. \\\n",
    "            nodes())[i1],list(community_structure.nodes())[i2], \\\n",
    "                                         weight=wweight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise the community of concepts\n",
    "\n",
    "The Sardinian Wikipedia, seen as a network of pages, reveals a dual nature: one is the communities of *abstract concepts* such as history, language, adminstrative organisation etc. The other is the mapping of the island and its villages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation section\n",
    "\n",
    "Graphviz and pydot are known to have recurrent issues with installation, especially on Windows.\n",
    "\n",
    "Test them out on your own platform before proceeding.\n",
    "\n",
    "Reference page: [networkx](https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pydot.graphviz_layout.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the simplest possible visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYOUT = 'circo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.complete_graph(4)\n",
    "\n",
    "pos_default = nx.nx_pydot.graphviz_layout(G)\n",
    "\n",
    "pos_circo = nx.nx_pydot.graphviz_layout(G, prog=LAYOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a visualisation layout, try them out; default is `neato`.<!-- .slide: data-fullscreen -->\n",
    "\n",
    "- `circo`: arranges nodes in circles\n",
    "\n",
    "- `dot`: hierarchical layout for DAGs\n",
    "\n",
    "- `neato`: the spring-model layout, it uses force-directed positioning\n",
    "\n",
    "- `fdp`: the same spring model but with *repulsive* force\n",
    "  \n",
    "- `sfdp`: multiscale version of the above, useful for large graphs\n",
    "  \n",
    "- `twopi`: the radial layout, it uses concentric circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYOUT = 'circo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.nx_pydot.graphviz_layout(community_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up your favorite visualisation\n",
    "node_size_factor = 2000.0\n",
    "\n",
    "edge_weight_factor = 10.0\n",
    "\n",
    "# axes not needed here\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for n in community_structure.nodes():\n",
    "    nx.draw_networkx_nodes(community_structure, pos, [n], node_size\\\n",
    "                           = node_size_factor*diz_node_sizes[n]/ \\\n",
    "                           max_node_size, node_color='Black')\n",
    "    nx.draw_networkx_labels(community_structure,pos, font_color= \\\n",
    "                            'White',axis='off')\n",
    "\n",
    "for e in community_structure.edges():\n",
    "    nx.draw_networkx_edges(community_structure,pos,[e],alpha=0.5, \\\n",
    "                           width=edge_weight_factor* \\\n",
    "                           community_structure[e[0]][e[1]]['weight']\\\n",
    "                           /max_edge_weight)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
